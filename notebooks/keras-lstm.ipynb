{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec # make use of pretrained embeddings\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils # for converting labels vectors to matrices in multi-class cases\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from evaluate import compute_performance, compute_performance_from_df\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations for E1 precedes E2: 123\n",
      "annotations for E2 precedes E1: 16\n",
      "Value -> Class: {0: 'None', 1: 'E1 precedes E2', 2: 'E2 precedes E1'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"../annotations.json\")\n",
    "\n",
    "# how many annotations exist with the positive labels of interest?\n",
    "print(\"annotations for E1 precedes E2: {}\".format((pd.read_json(\"../annotations.json\").relation == \"E1 precedes E2\").sum()))\n",
    "print(\"annotations for E2 precedes E1: {}\".format((pd.read_json(\"../annotations.json\").relation == \"E2 precedes E1\").sum()))\n",
    "\n",
    "# filter out bugs and empty relations\n",
    "data = data[(data.relation != \"Bug\") & (data.relation != \"\")]\n",
    "\n",
    "# set labels other than precedence to \"None\"\n",
    "label_to_value = {\n",
    "    'E1 precedes E2': 1,\n",
    "    'E2 precedes E1': 2,\n",
    "    'None': 0,\n",
    "    'E1 subsumes E2': 0, \n",
    "    'E2 subsumes E1': 0, \n",
    "    'Equivalent': 0, \n",
    "    'Other': 0\n",
    "}\n",
    "\n",
    "# value -> label\n",
    "value_to_label = {v:k for (k,v) in label_to_value.items() if k in {\"E1 precedes E2\", \"E2 precedes E1\", \"None\"}}\n",
    "\n",
    "data.relation = data.relation.replace(label_to_value)\n",
    "# we should now have only three kinds of labels in our relation annotations\n",
    "assert len(set(data.relation.values)) == 3\n",
    "print(\"Value -> Class: {}\".format(value_to_label))\n",
    "\n",
    "# TODO: split the data evenly among classes (training, dev, and test)\n",
    "# TODO: perform 5-fold cross validation with where each fold has the the three classes represented\n",
    "# text as input\n",
    "x = data.text.values\n",
    "# relations as labels\n",
    "y = data.relation.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789'\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot, base_filter, Tokenizer\n",
    "\n",
    "# remove numbers, punct (except for _), etc.\n",
    "custom_filter = \"!#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789'\" \n",
    "print(custom_filter)\n",
    "tk = Tokenizer(\n",
    "    # the maximum num. of words to retain\n",
    "    nb_words=None,\n",
    "    # the characters to filter out from the text\n",
    "    filters=custom_filter,\n",
    "    # whether or not to convert the text to lowercase\n",
    "    lower=True,\n",
    "    # the character to split on\n",
    "    split=\" \",\n",
    "    # whether or not to treat each character as a word\n",
    "    char_level=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tk.fit_on_texts(x)\n",
    "#one_hot(text, n, filters=base_filter(), lower=True, split=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text into sequences of term indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tk.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# the maximum size of a sequence\n",
    "max_len = 200\n",
    "\n",
    "x = sequence.pad_sequences(x, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "w2v_data = \"../pmc-openaccess-w2v.bin\"\n",
    "def create_embeddings_weights(w2v_vectors_file, tokenizer):\n",
    "    word2index = tk.word_index\n",
    "    # reverse index\n",
    "    index2word = {i:w for (w,i) in tk.word_index.items()}\n",
    "    max_size = len(index2word) + 1\n",
    "    # load w2v model\n",
    "    w2v = Word2Vec.load_word2vec_format(w2v_vectors_file, binary=True)\n",
    "    word_vector_dims = w2v.vector_size\n",
    "    embedding_weights = np.zeros((max_size, word_vector_dims))\n",
    "    \n",
    "    for i,w in index2word.items():\n",
    "        try:\n",
    "            embedding_weights[i,:] = w2v[w]\n",
    "        except:\n",
    "            print(\"{} not found\".format(w))\n",
    "    return (w2v, embedding_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained word embeddings\n",
    "\n",
    "We can initialize our network with pretrained word embeddings.  Here, we use embeddings generated using a `word2vec` model that was trained on the open access subset of PubMed retrieved in **????**, which contains over a million (**TODO: get exact number**) papers.  \n",
    "\n",
    "In preparing the text for `word2vec`, we chose to ignore the following sections in the `nxml` files: \"references\", \"materials\", \"methods\", and \"supplementary-material\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" not found\n",
      "atmmutation not found\n",
      "kgamma not found\n",
      "gammah not found\n",
      "piasgamma not found\n",
      "pkczetaii not found\n",
      "ralas not found\n",
      "raswt not found\n",
      "ikappak not found\n",
      "gigoux not found\n",
      "gresko not found\n",
      "deltarbd not found\n",
      "tovok not found\n",
      "hamamori not found\n"
     ]
    }
   ],
   "source": [
    "# get pretrained embeddings from w2v model\n",
    "(w2v, pretrained_weights) = create_embeddings_weights(w2v_data, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of word vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('htert', 0.715091347694397),\n",
       " ('tert', 0.7061598300933838),\n",
       " ('wrn', 0.614983320236206),\n",
       " ('dnmt', 0.6079218983650208),\n",
       " ('recql', 0.6020419597625732),\n",
       " ('telomere', 0.596019446849823),\n",
       " ('trf', 0.5914173126220703),\n",
       " ('dnapkcs', 0.5826644897460938),\n",
       " ('cdk', 0.5765817761421204),\n",
       " ('dyskerin', 0.5567993521690369)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['telomerase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set layer dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max features: 2990\n"
     ]
    }
   ],
   "source": [
    "# the maximum number of features to retain\n",
    "max_features = len(tk.word_index) + 1 # for mask\n",
    "print(\"Max features: {}\".format(max_features))\n",
    "\n",
    "# the number of samples to use for one update\n",
    "batch_size = 32\n",
    "# \n",
    "hidden_size = w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare classes\n",
    "\n",
    "We have 3 possible labels (\"E1 precedes E2\", \"E2 precedes E1\", or \"None\"), so we need to convert our length-normalized class label vectors to matrices of size $n$ classes.\n",
    "\n",
    "**Important**: _if you don't do this, the model will not learn!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y = np_utils.to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 780 samples, validate on 156 samples\n",
      "Epoch 1/8\n",
      "780/780 [==============================] - 20s - loss: 0.6581 - acc: 0.7769 - val_loss: 0.3256 - val_acc: 0.8910\n",
      "Epoch 2/8\n",
      "780/780 [==============================] - 20s - loss: 0.4914 - acc: 0.8256 - val_loss: 0.2736 - val_acc: 0.8846\n",
      "Epoch 3/8\n",
      "780/780 [==============================] - 22s - loss: 0.4543 - acc: 0.8333 - val_loss: 0.2354 - val_acc: 0.9038\n",
      "Epoch 4/8\n",
      "780/780 [==============================] - 19s - loss: 0.4175 - acc: 0.8397 - val_loss: 0.2179 - val_acc: 0.9359\n",
      "Epoch 5/8\n",
      "780/780 [==============================] - 19s - loss: 0.4083 - acc: 0.8333 - val_loss: 0.1970 - val_acc: 0.9167\n",
      "Epoch 6/8\n",
      "780/780 [==============================] - 21s - loss: 0.3490 - acc: 0.8551 - val_loss: 0.1881 - val_acc: 0.9167\n",
      "Epoch 7/8\n",
      "780/780 [==============================] - 19s - loss: 0.3348 - acc: 0.8615 - val_loss: 0.1553 - val_acc: 0.9295\n",
      "Epoch 8/8\n",
      "780/780 [==============================] - 18s - loss: 0.3306 - acc: 0.8692 - val_loss: 0.1573 - val_acc: 0.9103\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# build the embeddings layer\n",
    "embeddings = Embedding(\n",
    "    input_dim=max_features, \n",
    "    output_dim=hidden_size, \n",
    "    input_length=max_len, \n",
    "    W_regularizer=None,\n",
    "    #weights=None,\n",
    "    # use pretrained vectors\n",
    "    weights=[pretrained_weights],\n",
    "    dropout=0.2\n",
    ")\n",
    "model.add(embeddings)\n",
    "# build the lstm layer\n",
    "lstm = LSTM(\n",
    "    #input_dim=max_features,\n",
    "    output_dim=hidden_size, \n",
    "    dropout_W=0.2, \n",
    "    dropout_U=0.2, \n",
    "    return_sequences=False\n",
    ")\n",
    "model.add(lstm)\n",
    "model.add(Dropout(0.5))\n",
    "# size should be equal to the number of classes\n",
    "model.add(Dense(num_classes))\n",
    "# at the end of the day, we only want one label per input (hence softmax)\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# add early stopping to help avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='rmsprop', \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    # input\n",
    "    x, \n",
    "    # target labels\n",
    "    y=y, \n",
    "    # how many examples to consider at once\n",
    "    batch_size=batch_size, \n",
    "    # the number of epochs to train\n",
    "    nb_epoch=8,\n",
    "    # 0 for no logging, 1 for progress bar logging, 2 for one log line per epoch\n",
    "    verbose=1,\n",
    "    # the validation data to use,\n",
    "    #validation_data=(x_dev, y_dev),\n",
    "    # how much data to reserve for validation (takes n% starting at the end of the dataset)\n",
    "    validation_split=0.2,\n",
    "    # should the training data be shuffled?\n",
    "    shuffle=True,\n",
    "    # dict mapping classes to weight for scaling in loss function\n",
    "    class_weight=None,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.231070866417\n",
      "Test accuracy: 0.887179487485\n"
     ]
    }
   ],
   "source": [
    "performance = model.evaluate(x, y, batch_size=batch_size, verbose=0)\n",
    "loss, accuracy = performance[0], performance[-1]\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We can display a simple graph of the network's architecture using `dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 211.00 410.00\" width=\"211pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 207.004,-406 207.004,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 6545722728 -->\n",
       "<g class=\"node\" id=\"node1\"><title>6545722728</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 203.004,-401.5 203.004,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-379.3\">embedding_input_1 (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 6545722896 -->\n",
       "<g class=\"node\" id=\"node2\"><title>6545722896</title>\n",
       "<polygon fill=\"none\" points=\"16.7139,-292.5 16.7139,-328.5 186.29,-328.5 186.29,-292.5 16.7139,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-306.3\">embedding_1 (Embedding)</text>\n",
       "</g>\n",
       "<!-- 6545722728&#45;&gt;6545722896 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>6545722728-&gt;6545722896</title>\n",
       "<path d=\"M101.502,-365.313C101.502,-357.289 101.502,-347.547 101.502,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-338.529 101.502,-328.529 98.0021,-338.529 105.002,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6545723008 -->\n",
       "<g class=\"node\" id=\"node3\"><title>6545723008</title>\n",
       "<polygon fill=\"none\" points=\"49.3623,-219.5 49.3623,-255.5 153.642,-255.5 153.642,-219.5 49.3623,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-233.3\">lstm_1 (LSTM)</text>\n",
       "</g>\n",
       "<!-- 6545722896&#45;&gt;6545723008 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>6545722896-&gt;6545723008</title>\n",
       "<path d=\"M101.502,-292.313C101.502,-284.289 101.502,-274.547 101.502,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-265.529 101.502,-255.529 98.0021,-265.529 105.002,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6582733064 -->\n",
       "<g class=\"node\" id=\"node4\"><title>6582733064</title>\n",
       "<polygon fill=\"none\" points=\"34.9829,-146.5 34.9829,-182.5 168.021,-182.5 168.021,-146.5 34.9829,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-160.3\">dropout_1 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 6545723008&#45;&gt;6582733064 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>6545723008-&gt;6582733064</title>\n",
       "<path d=\"M101.502,-219.313C101.502,-211.289 101.502,-201.547 101.502,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-192.529 101.502,-182.529 98.0021,-192.529 105.002,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6591165440 -->\n",
       "<g class=\"node\" id=\"node5\"><title>6591165440</title>\n",
       "<polygon fill=\"none\" points=\"46.6587,-73.5 46.6587,-109.5 156.345,-109.5 156.345,-73.5 46.6587,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-87.3\">dense_1 (Dense)</text>\n",
       "</g>\n",
       "<!-- 6582733064&#45;&gt;6591165440 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>6582733064-&gt;6591165440</title>\n",
       "<path d=\"M101.502,-146.313C101.502,-138.289 101.502,-128.547 101.502,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-119.529 101.502,-109.529 98.0021,-119.529 105.002,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6591271096 -->\n",
       "<g class=\"node\" id=\"node6\"><title>6591271096</title>\n",
       "<polygon fill=\"none\" points=\"22.9414,-0.5 22.9414,-36.5 180.062,-36.5 180.062,-0.5 22.9414,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-14.3\">activation_1 (Activation)</text>\n",
       "</g>\n",
       "<!-- 6591165440&#45;&gt;6591271096 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>6591165440-&gt;6591271096</title>\n",
       "<path d=\"M101.502,-73.3129C101.502,-65.2895 101.502,-55.5475 101.502,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-46.5288 101.502,-36.5288 98.0021,-46.5289 105.002,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.visualize_util import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# from keras.utils.visualize_util import plot\n",
    "# plot(model, to_file='model.png')\n",
    "# plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare `LSTM` to classifier's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_predictions(predictions):\n",
    "    \"\"\"\n",
    "    converts values in a numpy array to their corresponding label\n",
    "    \"\"\"\n",
    "    label_LUT = {\n",
    "        0:\"None\",\n",
    "        1:\"E1 precedes E2\",\n",
    "        2:\"E2 precedes E1\"\n",
    "    }\n",
    "    for p in predictions:\n",
    "        yield label_LUT.get(p, \"None\")\n",
    "        \n",
    "def get_gold_labels(annotations_path):\n",
    "    data = pd.read_json(annotations_path)\n",
    "    # filter out bugs and empty relations\n",
    "    data = data[(data.relation != \"Bug\") & (data.relation != \"\")]\n",
    "    data.relation = data.relation.replace(label_to_value)\n",
    "    data.relation = data.relation.replace(value_to_label)\n",
    "    return data.relation.values\n",
    "            \n",
    "new_preds = list(convert_predictions(predictions))\n",
    "gold = get_gold_labels(\"../annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performance\n",
      "                                Class  Precision  Recall    F1  Support\n",
      "0                      E1 precedes E2       0.13    0.37  0.19      122\n",
      "1                      E2 precedes E1       0.00    0.00  0.00       16\n",
      "2                                None       0.79    0.53  0.63      641\n",
      "3  TOTAL (macro for positive classes)       0.06    0.18  0.10      138\n",
      "\n",
      "LSTM performance (with pretrained embeddings)\n",
      "                                Class  Precision  Recall    F1  Support\n",
      "0                      E1 precedes E2       0.98    0.37  0.53      123\n",
      "1                      E2 precedes E1       1.00    0.37  0.55       16\n",
      "2                                None       0.88    1.00  0.93      641\n",
      "3  TOTAL (macro for positive classes)       0.99    0.37  0.54      139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_performance = compute_performance(\"results.tsv\")\n",
    "\n",
    "print(\"Classifier performance\")\n",
    "print(classifier_performance.round(2))\n",
    "print()\n",
    "\n",
    "lstm_performance = compute_performance_from_df(pd.DataFrame({'Gold':gold, 'Predicted':new_preds}))\n",
    "\n",
    "print(\"LSTM performance (with pretrained embeddings)\")\n",
    "print(lstm_performance.round(2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# def load_data():\n",
    "#     # load your data using this function\n",
    "\n",
    "# def create model():\n",
    "#     # create your model using this function\n",
    "\n",
    "# def train_and_evaluate__model(model, data[train], labels[train], data[test], labels[test)):\n",
    "#     model.fit...\n",
    "#     # fit and evaluate here.\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     n_folds = 10\n",
    "#     data, labels, header_info = load_data()\n",
    "#     skf = StratifiedKFold(labels, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "#     for i, (train, test) in enumerate(skf):\n",
    "#             print \"Running Fold\", i+1, \"/\", n_folds\n",
    "#             model = None # Clearing the NN.\n",
    "#             model = create_model()\n",
    "#             train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
